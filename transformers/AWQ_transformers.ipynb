{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_D9kG_efts3"
   },
   "source": [
    "# Transformers 模型量化技术：AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE9IhcVyktah"
   },
   "source": [
    "![img](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/Thumbnail.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwsg6nCwoThm"
   },
   "source": [
    "在2023年6月，Ji Lin等人发表了论文[AWQ：Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/pdf/2306.00978.pdf)。\n",
    "\n",
    "这篇论文详细介绍了一种激活感知权重量化算法，可以用于压缩任何基于 Transformer 的语言模型，同时只有微小的性能下降。关于 AWQ 算法的详细介绍，见[MIT Han Song 教授分享](https://hanlab.mit.edu/projects/awq)。\n",
    "\n",
    "transformers 现在支持两个不同的 AWQ 开源实现库：\n",
    "\n",
    "- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)\n",
    "- [LLM-AWQ](https://github.com/mit-han-lab/llm-awq) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H2019RkoiM-"
   },
   "source": [
    "因为 LLM-AWQ 不支持 Nvidia T4 GPU（课程演示 GPU），所以我们使用 AutoAWQ 库来介绍和演示 AWQ 模型量化技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化前模型测试文本生成任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivanzhu/tools/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 651/651 [00:00<00:00, 8.64MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 251M/251M [00:23<00:00, 10.8MB/s] \n",
      "generation_config.json: 100%|██████████| 137/137 [00:00<00:00, 1.36MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 685/685 [00:00<00:00, 9.03MB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.92MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 771kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 441/441 [00:00<00:00, 6.33MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"facebook/opt-125m\"\n",
    "\n",
    "# 使用 GPU 加载原始的 OPT-125m 模型\n",
    "generator = pipeline('text-generation',\n",
    "                     model=model_path,\n",
    "                     device=0,\n",
    "                     do_sample=True,\n",
    "                     num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实测GPU显存占用：加载 OPT-125m 模型后\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 15:11:33 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   47C    P0              26W /  70W |    635MiB / 15360MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The woman worked as a truck driver at a dealership from 1988 through 1989. She is a nurse assistant'},\n",
       " {'generated_text': 'The woman worked as a janitor at the restaurant and went on to become an administrator in New York'},\n",
       " {'generated_text': 'The woman worked as a nurse for the University hospital in the city of New Philadelphia. She went on'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The woman worked as a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The man worked as a truck driver with a small business before joining the military, and now says he'},\n",
       " {'generated_text': 'The man worked as a security guard at a local bar. I’m guessing it was on'},\n",
       " {'generated_text': 'The man worked as a maintenance man for a garage and had no experience whatsoever.\\n>  \"'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The man worked as a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dJJRQ2p7eLQ"
   },
   "source": [
    "## 使用 AutoAWQ 量化模型\n",
    "\n",
    "下面我们以 `facebook opt-125m` 模型为例，使用 `AutoAWQ` 库实现的 AWQ 算法实现模型量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "LICENSE.md: 100%|██████████| 11.1k/11.1k [00:00<00:00, 98.4MB/s]\n",
      "\n",
      "README.md: 100%|██████████| 7.10k/7.10k [00:00<00:00, 80.9MB/s]\n",
      "\n",
      "\n",
      ".gitattributes: 100%|██████████| 1.17k/1.17k [00:00<00:00, 18.4MB/s]\n",
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 12.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "quant_path = \"models/opt-125m-awq\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "\n",
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "Qn_P_E5p7gAN",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 167/167 [00:00<00:00, 2.57MB/s]\n",
      "/home/ivanzhu/tools/miniconda3/lib/python3.11/site-packages/huggingface_hub-0.19.4-py3.8.egg/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|          | 4.19M/471M [00:03<06:12, 1.25MB/s]\u001b[A\n",
      "Downloading data:   3%|▎         | 12.6M/471M [00:04<02:18, 3.31MB/s]\u001b[A\n",
      "Downloading data:   4%|▍         | 21.0M/471M [00:05<01:35, 4.73MB/s]\u001b[A\n",
      "Downloading data:   6%|▌         | 29.4M/471M [00:06<01:17, 5.67MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 37.7M/471M [00:07<01:07, 6.38MB/s]\u001b[A\n",
      "Downloading data:  10%|▉         | 46.1M/471M [00:08<01:01, 6.87MB/s]\u001b[A\n",
      "Downloading data:  12%|█▏        | 54.5M/471M [00:09<00:58, 7.11MB/s]\u001b[A\n",
      "Downloading data:  13%|█▎        | 62.9M/471M [00:10<00:55, 7.33MB/s]\u001b[A\n",
      "Downloading data:  15%|█▌        | 71.3M/471M [00:11<00:53, 7.51MB/s]\u001b[A\n",
      "Downloading data:  17%|█▋        | 79.7M/471M [00:12<00:51, 7.62MB/s]\u001b[A\n",
      "Downloading data:  19%|█▊        | 88.1M/471M [00:13<00:49, 7.73MB/s]\u001b[A\n",
      "Downloading data:  20%|██        | 96.5M/471M [00:15<00:47, 7.82MB/s]\u001b[A\n",
      "Downloading data:  22%|██▏       | 105M/471M [00:16<00:49, 7.41MB/s] \u001b[A\n",
      "Downloading data:  24%|██▍       | 113M/471M [00:17<00:47, 7.59MB/s]\u001b[A\n",
      "Downloading data:  26%|██▌       | 122M/471M [00:18<00:45, 7.72MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 130M/471M [00:19<00:46, 7.29MB/s]\u001b[A\n",
      "Downloading data:  29%|██▉       | 138M/471M [00:20<00:44, 7.49MB/s]\u001b[A\n",
      "Downloading data:  31%|███       | 147M/471M [00:21<00:42, 7.63MB/s]\u001b[A\n",
      "Downloading data:  33%|███▎      | 155M/471M [00:22<00:40, 7.75MB/s]\u001b[A\n",
      "Downloading data:  35%|███▍      | 164M/471M [00:24<00:41, 7.48MB/s]\u001b[A\n",
      "Downloading data:  37%|███▋      | 172M/471M [00:25<00:39, 7.64MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 180M/471M [00:26<00:37, 7.68MB/s]\u001b[A\n",
      "Downloading data:  40%|████      | 189M/471M [00:27<00:36, 7.75MB/s]\u001b[A\n",
      "Downloading data:  42%|████▏     | 197M/471M [00:28<00:34, 7.83MB/s]\u001b[A\n",
      "Downloading data:  44%|████▎     | 206M/471M [00:29<00:33, 7.88MB/s]\u001b[A\n",
      "Downloading data:  45%|████▌     | 214M/471M [00:30<00:32, 7.92MB/s]\u001b[A\n",
      "Downloading data:  47%|████▋     | 222M/471M [00:31<00:31, 7.96MB/s]\u001b[A\n",
      "Downloading data:  49%|████▉     | 231M/471M [00:32<00:30, 7.97MB/s]\u001b[A\n",
      "Downloading data:  51%|█████     | 239M/471M [00:33<00:29, 7.99MB/s]\u001b[A\n",
      "Downloading data:  53%|█████▎    | 247M/471M [00:34<00:27, 8.00MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▍    | 256M/471M [00:35<00:26, 8.01MB/s]\u001b[A\n",
      "Downloading data:  56%|█████▌    | 264M/471M [00:36<00:25, 7.99MB/s]\u001b[A\n",
      "Downloading data:  58%|█████▊    | 273M/471M [00:37<00:24, 8.01MB/s]\u001b[A\n",
      "Downloading data:  60%|█████▉    | 281M/471M [00:38<00:23, 8.01MB/s]\u001b[A\n",
      "Downloading data:  61%|██████▏   | 289M/471M [00:39<00:22, 8.02MB/s]\u001b[A\n",
      "Downloading data:  63%|██████▎   | 298M/471M [00:40<00:21, 8.02MB/s]\u001b[A\n",
      "Downloading data:  65%|██████▌   | 306M/471M [00:41<00:20, 8.01MB/s]\u001b[A\n",
      "Downloading data:  67%|██████▋   | 315M/471M [00:42<00:19, 8.02MB/s]\u001b[A\n",
      "Downloading data:  69%|██████▊   | 323M/471M [00:43<00:18, 7.98MB/s]\u001b[A\n",
      "Downloading data:  70%|███████   | 331M/471M [00:45<00:18, 7.44MB/s]\u001b[A\n",
      "Downloading data:  72%|███████▏  | 340M/471M [00:46<00:17, 7.56MB/s]\u001b[A\n",
      "Downloading data:  74%|███████▍  | 348M/471M [00:47<00:16, 7.38MB/s]\u001b[A\n",
      "Downloading data:  76%|███████▌  | 357M/471M [00:48<00:16, 7.08MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 365M/471M [00:50<00:15, 6.98MB/s]\u001b[A\n",
      "Downloading data:  79%|███████▉  | 373M/471M [00:51<00:14, 6.70MB/s]\u001b[A\n",
      "Downloading data:  81%|████████  | 382M/471M [00:52<00:12, 6.99MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 390M/471M [00:53<00:11, 7.21MB/s]\u001b[A\n",
      "Downloading data:  85%|████████▍ | 398M/471M [00:54<00:09, 7.40MB/s]\u001b[A\n",
      "Downloading data:  86%|████████▋ | 407M/471M [00:55<00:08, 7.55MB/s]\u001b[A\n",
      "Downloading data:  88%|████████▊ | 415M/471M [00:56<00:07, 7.63MB/s]\u001b[A\n",
      "Downloading data:  90%|████████▉ | 424M/471M [00:58<00:06, 7.25MB/s]\u001b[A\n",
      "Downloading data:  92%|█████████▏| 432M/471M [00:59<00:05, 7.02MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▎| 440M/471M [01:00<00:04, 7.27MB/s]\u001b[A\n",
      "Downloading data:  95%|█████████▌| 449M/471M [01:01<00:02, 7.47MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████▋| 457M/471M [01:02<00:01, 7.39MB/s]\u001b[A\n",
      "Downloading data:  99%|█████████▉| 466M/471M [01:04<00:00, 6.86MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 471M/471M [01:04<00:00, 7.31MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [01:04<00:00, 64.44s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Generating validation split: 214670 examples [00:02, 99858.17 examples/s] \n",
      "AWQ: 100%|██████████| 12/12 [01:33<00:00,  7.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实测GPU显存使用：量化模型时峰值达到将近 4GB\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 15:12:50 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   48C    P0              32W /  70W |    3703MiB / 15360MiB |      2%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nVzKDBlP_6MV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuPLq9sa8EaN"
   },
   "source": [
    "#### Transformers 兼容性配置\n",
    "\n",
    "为了使`quant_config` 与 transformers 兼容，我们需要修改配置文件：`使用 Transformers.AwqConfig 来实例化量化模型配置`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KE8xjwlL8DnA"
   },
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/opt-125m-awq/tokenizer_config.json',\n",
       " 'models/opt-125m-awq/special_tokens_map.json',\n",
       " 'models/opt-125m-awq/vocab.json',\n",
       " 'models/opt-125m-awq/merges.txt',\n",
       " 'models/opt-125m-awq/added_tokens.json',\n",
       " 'models/opt-125m-awq/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)  # 保存分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 GPU 加载量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"cuda\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to hear you're doing well!\n",
      "Thank you! I'm glad to hear you're doing well!\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Z0hAXYanCDW3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman worked as a nurse at the hospital for a year. She was a nurse at the hospital for a year. She was a nurse at the hospital for a year. She was a nurse at the hospital for a year. She was a nurse at the hospital for a year. She was a nurse at the hospital for a year. She\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The woman worked as a\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework：使用 AWQ 量化 Facebook OPT-6.7B 模型\n",
    "\n",
    "Facebook OPT 模型：https://huggingface.co/facebook?search_models=opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
